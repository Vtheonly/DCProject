{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "34g2aljhX0-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a6d862-0294-47b5-f2dd-9e24e70cc846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Modular directory structure created successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Create Project Structure\n",
        "import os\n",
        "\n",
        "folders = [\n",
        "    \"wpf_engine\",\n",
        "    \"wpf_engine/config\",\n",
        "    \"wpf_engine/core\",\n",
        "    \"wpf_engine/data\",\n",
        "    \"wpf_engine/models\",\n",
        "    \"wpf_engine/pipelines\",\n",
        "    \"wpf_engine/utils\",\n",
        "    \"wpf_engine/scripts\",\n",
        "    \"saved_models\"\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    # Create __init__.py for module resolution\n",
        "    if \"saved_models\" not in folder:\n",
        "        with open(os.path.join(folder, \"__init__.py\"), \"w\") as f:\n",
        "            pass\n",
        "\n",
        "print(\"âœ… Modular directory structure created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Configuration & Logging (%%writefile wpf_engine/config/settings.py)\n",
        "%%writefile wpf_engine/config/settings.py\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    # Paths\n",
        "    DATA_PATH = \"wtbdata_245days.csv\"\n",
        "    SAVE_DIR = \"saved_models\"\n",
        "\n",
        "    # Spatio-Temporal Specs\n",
        "    LOOKBACK_STEPS = 144  # 24 hours of 10-min intervals\n",
        "    FORECAST_STEPS = 288  # 48 hours of 10-min intervals\n",
        "    TRAIN_STRIDE = 6      # Slide window by 1 hour during training to prevent RAM explosion\n",
        "\n",
        "    # Neural Architecture Config\n",
        "    BATCH_SIZE = 256\n",
        "    EPOCHS = 30\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Features\n",
        "    TARGET_COL = \"Patv\"\n",
        "    PHYSICS_FEATURES = [\"U\", \"V\", \"Energy_Flux\", \"Momentum\", \"Etmp\", \"Itmp\", \"Ndir\", \"Pab1\", \"Pab2\", \"Pab3\", \"Prtv\"]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_path(cls, version=\"v1\"):\n",
        "        return os.path.join(cls.SAVE_DIR, f\"physics_ai_engine_{version}.h5\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_scaler_path(cls, version=\"v1\"):\n",
        "        return os.path.join(cls.SAVE_DIR, f\"scaler_{version}.pkl\")"
      ],
      "metadata": {
        "id": "bRg7F3G2YU3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61539c8c-88a9-4c20-e242-8b36fd9617c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/config/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Robust Logger & Exceptions (%%writefile wpf_engine/utils/logger.py)\n",
        "%%writefile wpf_engine/utils/logger.py\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "def get_logger(name=\"WPF_Engine\"):\n",
        "    logger = logging.getLogger(name)\n",
        "    if not logger.handlers:\n",
        "        logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        ch = logging.StreamHandler(sys.stdout)\n",
        "        ch.setFormatter(formatter)\n",
        "        logger.addHandler(ch)\n",
        "\n",
        "        fh = logging.FileHandler(\"engine_run.log\")\n",
        "        fh.setFormatter(formatter)\n",
        "        logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "class DataIntegrityError(Exception):\n",
        "    \"\"\"Raised when SCADA data violates expected physics/rules boundaries.\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "Yg7DkEYzYY5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56355b5e-fb87-4216-b06b-4e505fe58b5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/utils/logger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Adapter & Physics Engine (%%writefile wpf_engine/data/physics_engine.py)\n",
        "%%writefile wpf_engine/data/physics_engine.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wpf_engine.utils.logger import get_logger\n",
        "\n",
        "logger = logging.getLogger(\"WPF_Engine\") if 'logging' in locals() else get_logger()\n",
        "\n",
        "class SDWPFAdapter:\n",
        "    \"\"\"Ingests raw SCADA logs, applies KDD Cup 2022 validity rules.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_scada_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        logger.info(\"Applying KDD Cup 2022 Boundary & Cleaning Rules...\")\n",
        "\n",
        "        # Rule 1: Patv < 0 -> Patv = 0\n",
        "        df['Patv'] = np.where(df['Patv'] < 0, 0, df['Patv'])\n",
        "\n",
        "        # Rule 2: Invalid Pitch Angles -> Nullify Target (Turbine Stopped/Abnormal)\n",
        "        invalid_pitch = (df['Pab1'] > 89) | (df['Pab2'] > 89) | (df['Pab3'] > 89)\n",
        "\n",
        "        # Rule 3: Abnormal Nacelle and Wind Directions\n",
        "        invalid_ndir = (df['Ndir'] > 720) | (df['Ndir'] < -720)\n",
        "        invalid_wdir = (df['Wdir'] > 180) | (df['Wdir'] < -180)\n",
        "\n",
        "        # Combine rules and mask abnormal targets\n",
        "        abnormal_mask = invalid_pitch | invalid_ndir | invalid_wdir\n",
        "        df.loc[abnormal_mask, 'Patv'] = np.nan\n",
        "\n",
        "        # Forward fill missing values (up to a limit) to maintain sequence integrity\n",
        "        df.ffill(limit=6, inplace=True) # Max 1 hour of imputation\n",
        "        df.bfill(inplace=True)\n",
        "        return df\n",
        "\n",
        "class PhysicsEngine:\n",
        "    \"\"\"Transforms raw meteorological readings into continuous physics states.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_physics(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        logger.info(\"Igniting Physics Engine: Calculating vectors, flux, and momentum...\")\n",
        "\n",
        "        # 1. Wind Vectors (U, V) - Resolves the 359/1 degree circular boundary AI confusion\n",
        "        wind_dir_rad = np.deg2rad(df['Wdir'])\n",
        "        df['U'] = df['Wspd'] * np.cos(wind_dir_rad)\n",
        "        df['V'] = df['Wspd'] * np.sin(wind_dir_rad)\n",
        "\n",
        "        # 2. Energy Flux (V^3) - Wind power is proportional to the cube of velocity\n",
        "        df['Energy_Flux'] = np.power(df['Wspd'], 3)\n",
        "\n",
        "        # 3. Momentum (Delta V) - Acceleration indicates incoming gusts/fronts\n",
        "        # Group by TurbID to prevent cross-turbine momentum calculations\n",
        "        df['Momentum'] = df.groupby('TurbID')['Wspd'].diff().fillna(0)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "drw2pWJTYcrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b190cd1e-86e2-4883-fec2-6ab1722fbec3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/data/physics_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Vectorized Sequence Generator (%%writefile wpf_engine/data/sequence_generator.py)\n",
        "%%writefile wpf_engine/data/sequence_generator.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class VectorizedSlicer:\n",
        "    \"\"\"Generates 3D tensors natively in RAM, respecting turbine boundaries.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.target_col = Config.TARGET_COL\n",
        "        self.feature_cols = Config.PHYSICS_FEATURES\n",
        "\n",
        "    def fit_transform_scale(self, df: pd.DataFrame, is_train: bool = True):\n",
        "        logger.info(\"Scaling features...\")\n",
        "        if is_train:\n",
        "            df[self.feature_cols] = self.scaler.fit_transform(df[self.feature_cols])\n",
        "            joblib.dump(self.scaler, Config.get_scaler_path())\n",
        "        else:\n",
        "            self.scaler = joblib.load(Config.get_scaler_path())\n",
        "            df[self.feature_cols] = self.scaler.transform(df[self.feature_cols])\n",
        "        return df\n",
        "\n",
        "    def create_sequences(self, df: pd.DataFrame):\n",
        "        logger.info(\"Executing Vectorized RAM Slicing per Turbine...\")\n",
        "        X, Y, Anchors = [], [], []\n",
        "\n",
        "        grouped = df.groupby('TurbID')\n",
        "\n",
        "        for turb_id, group in grouped:\n",
        "            group = group.reset_index(drop=True)\n",
        "            values_x = group[self.feature_cols].values\n",
        "            values_y = group[self.target_col].values\n",
        "\n",
        "            # Using strided window logic\n",
        "            num_samples = len(group) - Config.LOOKBACK_STEPS - Config.FORECAST_STEPS\n",
        "            if num_samples <= 0:\n",
        "                continue\n",
        "\n",
        "            for i in range(0, num_samples, Config.TRAIN_STRIDE):\n",
        "                # Historical Features\n",
        "                x_seq = values_x[i : i + Config.LOOKBACK_STEPS]\n",
        "                # Target Future Actual Power\n",
        "                y_seq = values_y[i + Config.LOOKBACK_STEPS : i + Config.LOOKBACK_STEPS + Config.FORECAST_STEPS]\n",
        "                # The \"Power Anchor\": Last known power value at the end of lookback\n",
        "                anchor = values_y[i + Config.LOOKBACK_STEPS - 1]\n",
        "\n",
        "                # We skip sequences where the target has NaNs (due to invalid rules)\n",
        "                if np.isnan(y_seq).any() or np.isnan(anchor):\n",
        "                    continue\n",
        "\n",
        "                X.append(x_seq)\n",
        "                Y.append(y_seq)\n",
        "                Anchors.append([anchor]) # Shape (1,)\n",
        "\n",
        "        logger.info(f\"Generated {len(X)} sequences.\")\n",
        "        return np.array(X, dtype=np.float32), np.array(Anchors, dtype=np.float32), np.array(Y, dtype=np.float32)"
      ],
      "metadata": {
        "id": "3uMoKJK5Yj4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e93147-075d-4fd0-a3a0-88fcba07bb44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/data/sequence_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Core AI Models (%%writefile wpf_engine/models/hybrid_model.py)\n",
        "%%writefile wpf_engine/models/hybrid_model.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class DifferentialTransformer:\n",
        "    @staticmethod\n",
        "    def build_model(input_shape=(Config.LOOKBACK_STEPS, len(Config.PHYSICS_FEATURES))):\n",
        "        logger.info(\"Constructing Residual Differential Transformer...\")\n",
        "\n",
        "        # Input 1: Historical Physics Features\n",
        "        inputs_seq = layers.Input(shape=input_shape, name=\"historical_physics\")\n",
        "\n",
        "        # Input 2: Power Anchor (The exact power output at time T=0)\n",
        "        input_anchor = layers.Input(shape=(1,), name=\"power_anchor\")\n",
        "\n",
        "        # Stage 1: Edge Detection (Gusts)\n",
        "        x = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs_seq)\n",
        "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "        # Stage 2: Temporal Memory (Weather Fronts)\n",
        "        x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
        "\n",
        "        # Stage 3: Attention Mechanism (Focusing on cyclical patterns)\n",
        "        attention = layers.MultiHeadAttention(num_heads=4, key_dim=128)(x, x)\n",
        "        x = layers.Add()([x, attention]) # Residual connection\n",
        "        x = layers.LayerNormalization()(x)\n",
        "\n",
        "        # Collapse sequence to vector\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Stage 4: Differential Head (Predicting Delta -> Change in Power)\n",
        "        x = layers.Dense(256, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        delta_power = layers.Dense(Config.FORECAST_STEPS, activation=\"linear\", name=\"delta_prediction\")(x)\n",
        "\n",
        "        # Stage 5: The \"Game Changing\" Anchor Addition\n",
        "        # Expand anchor to match forecast horizon: shape (Batch, 288)\n",
        "        repeated_anchor = layers.RepeatVector(Config.FORECAST_STEPS)(input_anchor)\n",
        "        repeated_anchor = layers.Reshape((Config.FORECAST_STEPS,))(repeated_anchor)\n",
        "\n",
        "        # Final Output = Present Power + Predicted Change\n",
        "        final_prediction = layers.Add(name=\"final_absolute_power\")([repeated_anchor, delta_power])\n",
        "\n",
        "        # Enforce physical constraints (Power cannot realistically be < 0)\n",
        "        final_prediction = layers.ReLU()(final_prediction)\n",
        "\n",
        "        model = Model(inputs=[inputs_seq, input_anchor], outputs=final_prediction)\n",
        "\n",
        "        # We use MSE for strict gradient descent, but log MAE for human readability\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE),\n",
        "                      loss='mse',\n",
        "                      metrics=['mae'])\n",
        "        return model"
      ],
      "metadata": {
        "id": "5zP7oqDYYmD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589fc5b5-9fcb-43da-887a-0e5cab4cd4f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/models/hybrid_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Training Pipeline (%%writefile wpf_engine/pipelines/trainer.py)\n",
        "%%writefile wpf_engine/pipelines/trainer.py\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.data.physics_engine import SDWPFAdapter, PhysicsEngine\n",
        "from wpf_engine.data.sequence_generator import VectorizedSlicer\n",
        "from wpf_engine.models.hybrid_model import DifferentialTransformer\n",
        "from wpf_engine.utils.logger import get_logger\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class EnginePipeline:\n",
        "    def __init__(self):\n",
        "        self.slicer = VectorizedSlicer()\n",
        "\n",
        "    def run_training(self):\n",
        "        logger.info(f\"Loading Raw SCADA Data from {Config.DATA_PATH}...\")\n",
        "        try:\n",
        "            # We sample chunk if dataset is massive to prevent Colab crashing initially,\n",
        "            # though 250MB should fit. Adjust nrows if needed.\n",
        "            df = pd.read_csv(Config.DATA_PATH)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Dataset not found at {Config.DATA_PATH}. Please upload it to Colab.\")\n",
        "            return\n",
        "\n",
        "        # 1. Pipeline Ingestion\n",
        "        df = SDWPFAdapter.clean_scada_data(df)\n",
        "        df = PhysicsEngine.apply_physics(df)\n",
        "        df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "        # 2. Scaling\n",
        "        df = self.slicer.fit_transform_scale(df, is_train=True)\n",
        "\n",
        "        # 3. RAM Slicing\n",
        "        X, Anchors, Y = self.slicer.create_sequences(df)\n",
        "\n",
        "        if len(X) == 0:\n",
        "            logger.error(\"No valid sequences generated. Check data integrity.\")\n",
        "            return\n",
        "\n",
        "        # 4. Initialize Brain\n",
        "        model = DifferentialTransformer.build_model()\n",
        "        model.summary(print_fn=logger.info)\n",
        "\n",
        "        # 5. Callbacks for Industrial Robustness\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_mae'),\n",
        "            ModelCheckpoint(Config.get_model_path(), save_best_only=True, monitor='val_mae'),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=2, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # 6. Execution\n",
        "        logger.info(\"Initializing GPU Training Loop...\")\n",
        "        history = model.fit(\n",
        "            x=[X, Anchors],\n",
        "            y=Y,\n",
        "            batch_size=Config.BATCH_SIZE,\n",
        "            epochs=Config.EPOCHS,\n",
        "            validation_split=0.15,\n",
        "            callbacks=callbacks,\n",
        "            shuffle=True\n",
        "        )\n",
        "        logger.info(f\"Training Complete. Model securely locked at {Config.get_model_path()}\")"
      ],
      "metadata": {
        "id": "rEhrfCpvYwCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628536b6-834c-4545-e09a-650106a5a321"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/pipelines/trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Execution Script (%%writefile run_engine.py)\n",
        "%%writefile run_engine.py\n",
        "from wpf_engine.pipelines.trainer import EnginePipeline\n",
        "from wpf_engine.utils.logger import get_logger\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger = get_logger()\n",
        "    logger.info(\"=========================================\")\n",
        "    logger.info(\"  SPATIO-TEMPORAL PHYSICS-AI ENGINE V1.0 \")\n",
        "    logger.info(\"=========================================\")\n",
        "\n",
        "    pipeline = EnginePipeline()\n",
        "    pipeline.run_training()\n",
        "\n",
        "    logger.info(\"Engine Shutdown Successfully.\")"
      ],
      "metadata": {
        "id": "G4ZpYH4eY3US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de698e60-2444-4b4f-d644-431e3325829c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Initialize System & Directories\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Clean slate approach\n",
        "if os.path.exists(\"wpf_engine\"):\n",
        "    shutil.rmtree(\"wpf_engine\")\n",
        "\n",
        "structure = [\n",
        "    \"wpf_engine\",\n",
        "    \"wpf_engine/config\",\n",
        "    \"wpf_engine/core\",  # For evaluation and logic\n",
        "    \"wpf_engine/data\",  # For ingestion and generators\n",
        "    \"wpf_engine/models\",\n",
        "    \"wpf_engine/pipelines\",\n",
        "    \"wpf_engine/utils\",\n",
        "    \"saved_models\",\n",
        "    \"saved_models/plots\",\n",
        "    \"saved_models/logs\"\n",
        "]\n",
        "\n",
        "for folder in structure:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    if \"saved_models\" not in folder:\n",
        "        with open(os.path.join(folder, \"__init__.py\"), \"w\") as f:\n",
        "            pass\n",
        "\n",
        "print(\"âœ… Enterprise Architecture deployed successfully.\")"
      ],
      "metadata": {
        "id": "GAjWzBbvZpL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066aee42-106a-48a0-f2a8-2eb0c6bbe238"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enterprise Architecture deployed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Configuration (%%writefile wpf_engine/config/settings.py)\n",
        "%%writefile wpf_engine/config/settings.py\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    # -- Data Paths --\n",
        "    DATA_PATH = \"wtbdata_245days.csv\"\n",
        "    ARTIFACTS_DIR = \"saved_models\"\n",
        "    PLOT_DIR = os.path.join(ARTIFACTS_DIR, \"plots\")\n",
        "\n",
        "    # -- Physics & Time --\n",
        "    LOOKBACK_STEPS = 144   # 24 hours (144 * 10min)\n",
        "    FORECAST_STEPS = 288   # 48 hours (288 * 10min)\n",
        "\n",
        "    # -- Training Hyperparameters --\n",
        "    BATCH_SIZE = 256       # Large batch for stable gradients\n",
        "    EPOCHS = 20            # Reduced epochs because dataset is massive (4.7M rows)\n",
        "    LEARNING_RATE = 1e-3\n",
        "    TRAIN_TEST_SPLIT = 0.9 # First 90% days for train, last 10% for test\n",
        "\n",
        "    # -- Feature Definitions --\n",
        "    TARGET_COL = \"Patv\"\n",
        "    # Core physics features calculated by the engine\n",
        "    PHYSICS_FEATURES = [\n",
        "        \"Wspd\", \"Wdir\", \"Etmp\", \"Itmp\", \"Ndir\", \"Pab1\", \"Pab2\", \"Pab3\", \"Prtv\",\n",
        "        \"Patv\", \"U\", \"V\", \"Energy_Flux\", \"Momentum\"\n",
        "    ]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"deep_physics_transformer.h5\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_scaler_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"global_scaler.pkl\")"
      ],
      "metadata": {
        "id": "7OXKarHaZtY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301c7ba5-b14c-401a-97a7-2b0f85afba39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/config/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Data Processing (%%writefile wpf_engine/data/processor.py)\n",
        "%%writefile wpf_engine/data/processor.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "class PhysicsEngine:\n",
        "    @staticmethod\n",
        "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        print(\"âš¡ Igniting Physics Engine...\")\n",
        "\n",
        "        # 1. Clean Data (KDD Rules)\n",
        "        df['Patv'] = df['Patv'].clip(lower=0) # Negative power is impossible\n",
        "\n",
        "        # 2. Vector Decomposition (Solves the 0-360 degree discontinuity)\n",
        "        wdir_rad = np.deg2rad(df['Wdir'])\n",
        "        df['U'] = df['Wspd'] * np.cos(wdir_rad)\n",
        "        df['V'] = df['Wspd'] * np.sin(wdir_rad)\n",
        "\n",
        "        # 3. Energy Flux (The cubic relationship: Power ~ V^3)\n",
        "        df['Energy_Flux'] = np.power(df['Wspd'], 3)\n",
        "\n",
        "        # 4. Momentum (Derivative of wind speed - acceleration)\n",
        "        # Using specific group-by to avoid bleeding data between turbines\n",
        "        df['Momentum'] = df.groupby('TurbID')['Wspd'].diff().fillna(0)\n",
        "\n",
        "        # 5. Handle Missing values securely (Forward fill then Back fill)\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        return df\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        # Fit scaler on everything to ensure global min/max coverage\n",
        "        print(\"âš–ï¸ Scaling 4.7M+ rows (MinMax)...\")\n",
        "        df[Config.PHYSICS_FEATURES] = self.scaler.fit_transform(df[Config.PHYSICS_FEATURES])\n",
        "        joblib.dump(self.scaler, Config.get_scaler_path())\n",
        "        return df, self.scaler\n",
        "\n",
        "class TimeSeriesGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Zero-Memory-Footprint Generator.\n",
        "    Instead of creating a 100GB array, we generate batches on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, indices, batch_size):\n",
        "        self.df = df\n",
        "        self.indices = indices\n",
        "        self.batch_size = batch_size\n",
        "        self.lookback = Config.LOOKBACK_STEPS\n",
        "        self.forecast = Config.FORECAST_STEPS\n",
        "        self.feature_cols = Config.PHYSICS_FEATURES\n",
        "        self.target_col_idx = df.columns.get_loc(Config.TARGET_COL)\n",
        "        self.feature_indices = [df.columns.get_loc(c) for c in self.feature_cols]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get batch of start indices\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "        X_hist, X_anchor, Y_future = [], [], []\n",
        "\n",
        "        data_array = self.df.values # Access numpy backing once for speed\n",
        "\n",
        "        for i in batch_indices:\n",
        "            # 1. Historical Context (0 to t)\n",
        "            # Shape: (144, Features)\n",
        "            x_seq = data_array[i : i + self.lookback, self.feature_indices]\n",
        "\n",
        "            # 2. Power Anchor (at time t)\n",
        "            # The value the model adds the delta to.\n",
        "            # Shape: (1,)\n",
        "            anchor = data_array[i + self.lookback - 1, self.target_col_idx]\n",
        "\n",
        "            # 3. Future Target (t+1 to t+288)\n",
        "            # Shape: (288,)\n",
        "            y_seq = data_array[i + self.lookback : i + self.lookback + self.forecast, self.target_col_idx]\n",
        "\n",
        "            X_hist.append(x_seq)\n",
        "            X_anchor.append(anchor)\n",
        "            Y_future.append(y_seq)\n",
        "\n",
        "        return [np.array(X_hist), np.array(X_anchor)], np.array(Y_future)"
      ],
      "metadata": {
        "id": "OE8dgTanZzZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41158db-bc05-4a7c-e449-816e532c8913"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/data/processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Model Architecture (%%writefile wpf_engine/models/architecture.py)\n",
        "%%writefile wpf_engine/models/architecture.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "def build_differential_model():\n",
        "    # --- Input 1: The Timeline (History) ---\n",
        "    input_seq = layers.Input(shape=(Config.LOOKBACK_STEPS, len(Config.PHYSICS_FEATURES)), name=\"history_in\")\n",
        "\n",
        "    # --- Input 2: The Anchor (Current Power) ---\n",
        "    input_anchor = layers.Input(shape=(1,), name=\"anchor_in\")\n",
        "\n",
        "    # 1. Feature Extraction (CNN for local patterns/gusts)\n",
        "    x = layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(input_seq)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(2)(x)\n",
        "\n",
        "    # 2. Temporal Dynamics (Bi-LSTM for long-term memory)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
        "\n",
        "    # 3. Differential Prediction Head\n",
        "    # We predict the *change* relative to the anchor, not absolute values.\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Output: Delta for 288 steps\n",
        "    delta_pred = layers.Dense(Config.FORECAST_STEPS, name=\"delta_out\")(x)\n",
        "\n",
        "    # 4. Physics Reconstruction (Anchor + Delta)\n",
        "    # Broadcast anchor to (Batch, 288)\n",
        "    anchor_broadcast = layers.RepeatVector(Config.FORECAST_STEPS)(input_anchor)\n",
        "    anchor_broadcast = layers.Reshape((Config.FORECAST_STEPS,))(anchor_broadcast)\n",
        "\n",
        "    # Final = Anchor + Delta\n",
        "    final_pred = layers.Add(name=\"reconstruction\")([anchor_broadcast, delta_pred])\n",
        "\n",
        "    # Physics Constraint: Power >= 0\n",
        "    final_pred = layers.ReLU()(final_pred)\n",
        "\n",
        "    model = Model(inputs=[input_seq, input_anchor], outputs=final_pred)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TeRa7BZqZ5IF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278ec72a-3e6a-4ac1-f270-5ee2871bc848"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/models/architecture.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Evaluation & Viz (%%writefile wpf_engine/core/evaluator.py)\n",
        "%%writefile wpf_engine/core/evaluator.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from wpf_engine.config.settings import Config\n",
        "import os\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, model, scaler):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        # We need the min/max of the Target column to un-scale predictions for real metrics\n",
        "        self.target_min = scaler.min_[Config.PHYSICS_FEATURES.index(Config.TARGET_COL)]\n",
        "        self.target_scale = scaler.scale_[Config.PHYSICS_FEATURES.index(Config.TARGET_COL)]\n",
        "\n",
        "    def unscale(self, data):\n",
        "        \"\"\"Converts normalized 0-1 data back to MW/kW.\"\"\"\n",
        "        return (data - self.target_min) / self.target_scale\n",
        "\n",
        "    def evaluate_and_plot(self, test_gen):\n",
        "        print(\"ðŸ“Š Running Comprehensive Evaluation...\")\n",
        "\n",
        "        # 1. Generate Predictions\n",
        "        # We take a subset of the test generator for visualization (e.g., first 50 batches)\n",
        "        # to avoid OOM during plotting, but calculate metrics on more.\n",
        "\n",
        "        all_trues = []\n",
        "        all_preds = []\n",
        "\n",
        "        # Iterate over validation set\n",
        "        steps_to_eval = min(len(test_gen), 100) # Evaluate on ~25k samples\n",
        "        print(f\"   Processing {steps_to_eval} batches...\")\n",
        "\n",
        "        for i in range(steps_to_eval):\n",
        "            x, y = test_gen[i]\n",
        "            preds = self.model.predict(x, verbose=0)\n",
        "            all_trues.append(y)\n",
        "            all_preds.append(preds)\n",
        "\n",
        "        # Flatten\n",
        "        y_true = np.concatenate(all_trues, axis=0)\n",
        "        y_pred = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "        # Unscale\n",
        "        # Since we used MinMaxScaler, X_std = (X - min) / (max - min)\n",
        "        # X = X_std * (max - min) + min\n",
        "        # Sklearn: data = (val - min) / scale  ->  val = data * scale + min -> Wait, scale is (max-min) or 1/(max-min)?\n",
        "        # Sklearn `scale_` attribute is (max-min). Actually: X_scaled = X * scale_ + min_\n",
        "        # Correction: inverse_transform is safer.\n",
        "\n",
        "        # Manual inverse for speed on just target\n",
        "        y_true_real = (y_true - self.scaler.min_[9]) / self.scaler.scale_[9]\n",
        "        y_pred_real = (y_pred - self.scaler.min_[9]) / self.scaler.scale_[9]\n",
        "\n",
        "        # --- Metrics ---\n",
        "        mse = mean_squared_error(y_true_real, y_pred_real)\n",
        "        mae = mean_absolute_error(y_true_real, y_pred_real)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_true_real.flatten(), y_pred_real.flatten())\n",
        "\n",
        "        print(f\"\\nðŸ† FINAL RESULTS:\")\n",
        "        print(f\"   RÂ² Score: {r2:.4f}\")\n",
        "        print(f\"   RMSE:     {rmse:.4f} kW\")\n",
        "        print(f\"   MAE:      {mae:.4f} kW\")\n",
        "        print(f\"   MSE:      {mse:.4f}\")\n",
        "\n",
        "        self.generate_plots(y_true_real, y_pred_real)\n",
        "\n",
        "        return {\"r2\": r2, \"rmse\": rmse, \"mae\": mae}\n",
        "\n",
        "    def generate_plots(self, y_true, y_pred):\n",
        "        plot_dir = Config.PLOT_DIR\n",
        "\n",
        "        # Plot 1: Prediction vs Truth (Sample)\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        # Plot random sample\n",
        "        idx = np.random.randint(0, len(y_true))\n",
        "        plt.plot(y_true[idx], label='Ground Truth (Real Physics)', color='black', linewidth=2)\n",
        "        plt.plot(y_pred[idx], label='AI Prediction', color='dodgerblue', linestyle='--')\n",
        "        plt.title(f\"48-Hour Forecast Horizon (Sample #{idx})\")\n",
        "        plt.xlabel(\"Time Steps (10-min intervals)\")\n",
        "        plt.ylabel(\"Active Power (kW)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(f\"{plot_dir}/1_forecast_sample.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Plot 2: Error Distribution\n",
        "        errors = (y_true - y_pred).flatten()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(errors, bins=100, kde=True, color='crimson')\n",
        "        plt.title(\"Error Distribution (Residuals)\")\n",
        "        plt.xlabel(\"Prediction Error (kW)\")\n",
        "        plt.savefig(f\"{plot_dir}/2_error_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Plot 3: Performance Per Horizon Step (Does it get worse over time?)\n",
        "        # Calculate RMSE per time step (0 to 288)\n",
        "        rmse_per_step = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(rmse_per_step, color='purple')\n",
        "        plt.title(\"RMSE Degradation over 48-Hour Horizon\")\n",
        "        plt.xlabel(\"Forecast Step (10m)\")\n",
        "        plt.ylabel(\"RMSE (kW)\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"{plot_dir}/3_rmse_per_step.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Plot 4: Scatter Density\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        # Sample points to keep plot light\n",
        "        flat_true = y_true.flatten()\n",
        "        flat_pred = y_pred.flatten()\n",
        "        indices = np.random.choice(len(flat_true), size=10000, replace=False)\n",
        "        plt.scatter(flat_true[indices], flat_pred[indices], alpha=0.1, s=1, c='green')\n",
        "        plt.plot([0, flat_true.max()], [0, flat_true.max()], 'r--')\n",
        "        plt.xlabel(\"Actual Power\")\n",
        "        plt.ylabel(\"Predicted Power\")\n",
        "        plt.title(\"Prediction Alignment (RÂ² visual)\")\n",
        "        plt.savefig(f\"{plot_dir}/4_scatter_alignment.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"ðŸŽ¨ Plots saved to {plot_dir}/\")"
      ],
      "metadata": {
        "id": "Nn9_ETdmZ8A8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960d8783-5b5a-4be7-f926-739f3100a132"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/core/evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Training Pipeline (%%writefile wpf_engine/pipelines/run.py)\n",
        "%%writefile wpf_engine/pipelines/run.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.data.processor import PhysicsEngine, DataManager, TimeSeriesGenerator\n",
        "from wpf_engine.models.architecture import build_differential_model\n",
        "from wpf_engine.core.evaluator import Evaluator\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸš€ Initializing Full Scale WPF Engine...\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        df = pd.read_csv(Config.DATA_PATH)\n",
        "        print(f\"ðŸ“‚ Dataset loaded: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ Dataset not found. Please upload 'wtbdata_245days.csv'\")\n",
        "        return\n",
        "\n",
        "    # 2. Physics & Scaling\n",
        "    df = PhysicsEngine.engineer_features(df)\n",
        "    manager = DataManager()\n",
        "    df, scaler = manager.prepare_data(df)\n",
        "\n",
        "    # 3. Create Valid Indices for Generator (Crucial for Speed)\n",
        "    # We filter indices where a full sequence (Lookback + Forecast) stays within one Turbine\n",
        "    print(\"âš™ï¸ Computing valid sequence indices...\")\n",
        "    # Group by TurbID and get valid start indices\n",
        "    groups = df.groupby('TurbID')\n",
        "    valid_indices = []\n",
        "\n",
        "    total_len = Config.LOOKBACK_STEPS + Config.FORECAST_STEPS\n",
        "\n",
        "    # Global index tracking\n",
        "    for turb_id, group in groups:\n",
        "        # Get global indices of this group\n",
        "        g_indices = group.index.values\n",
        "        # Determine how many valid sequences fit\n",
        "        n_samples = len(g_indices) - total_len\n",
        "        if n_samples > 0:\n",
        "            # We add the start indices.\n",
        "            # TimeSeriesGenerator will calculate i+lookback based on these.\n",
        "            valid_starts = g_indices[:n_samples]\n",
        "            valid_indices.extend(valid_starts)\n",
        "\n",
        "    valid_indices = np.array(valid_indices)\n",
        "    np.random.shuffle(valid_indices)\n",
        "\n",
        "    print(f\"âœ… Found {len(valid_indices)} valid sequences.\")\n",
        "\n",
        "    # 4. Train/Test Split\n",
        "    split_idx = int(len(valid_indices) * Config.TRAIN_TEST_SPLIT)\n",
        "    train_idx = valid_indices[:split_idx]\n",
        "    test_idx = valid_indices[split_idx:]\n",
        "\n",
        "    train_gen = TimeSeriesGenerator(df, train_idx, Config.BATCH_SIZE)\n",
        "    test_gen = TimeSeriesGenerator(df, test_idx, Config.BATCH_SIZE)\n",
        "\n",
        "    # 5. Build & Train\n",
        "    model = build_differential_model()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint(Config.get_model_path(), save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    print(\"ðŸ”¥ Starting GPU Training...\")\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=test_gen,\n",
        "        epochs=Config.EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # 6. Evaluate\n",
        "    evaluator = Evaluator(model, scaler)\n",
        "    metrics = evaluator.evaluate_and_plot(test_gen)\n",
        "\n",
        "    print(\"\\nâœ… Engine Run Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "jNhd8wsdZ_ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a1b7a5-174d-4072-f9f7-02d03f0195f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wpf_engine/pipelines/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Fix Pathing in Pipeline Script (%%writefile wpf_engine/pipelines/run.py)\n",
        "%%writefile wpf_engine/pipelines/run.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# --- PATH PATCH: Auto-detect Root Directory ---\n",
        "# This allows the script to be run directly from anywhere\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__)) # .../wpf_engine/pipelines\n",
        "root_dir = os.path.abspath(os.path.join(current_dir, '../..')) # .../\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "# ----------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.data.processor import PhysicsEngine, DataManager, TimeSeriesGenerator\n",
        "from wpf_engine.models.architecture import build_differential_model\n",
        "from wpf_engine.core.evaluator import Evaluator\n",
        "\n",
        "def main():\n",
        "    print(f\"ðŸš€ Initializing Full Scale WPF Engine (Root: {root_dir})...\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        df = pd.read_csv(Config.DATA_PATH)\n",
        "        print(f\"ðŸ“‚ Dataset loaded: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Dataset not found at {Config.DATA_PATH}. Please upload 'wtbdata_245days.csv' to Colab Files.\")\n",
        "        return\n",
        "\n",
        "    # 2. Physics & Scaling\n",
        "    df = PhysicsEngine.engineer_features(df)\n",
        "    manager = DataManager()\n",
        "    df, scaler = manager.prepare_data(df)\n",
        "\n",
        "    # 3. Create Valid Indices for Generator (Crucial for Speed)\n",
        "    # We filter indices where a full sequence (Lookback + Forecast) stays within one Turbine\n",
        "    print(\"âš™ï¸ Computing valid sequence indices...\")\n",
        "    # Group by TurbID and get valid start indices\n",
        "    groups = df.groupby('TurbID')\n",
        "    valid_indices = []\n",
        "\n",
        "    total_len = Config.LOOKBACK_STEPS + Config.FORECAST_STEPS\n",
        "\n",
        "    # Global index tracking\n",
        "    for turb_id, group in groups:\n",
        "        # Get global indices of this group\n",
        "        g_indices = group.index.values\n",
        "        # Determine how many valid sequences fit\n",
        "        n_samples = len(g_indices) - total_len\n",
        "        if n_samples > 0:\n",
        "            # We add the start indices.\n",
        "            # TimeSeriesGenerator will calculate i+lookback based on these.\n",
        "            valid_starts = g_indices[:n_samples]\n",
        "            valid_indices.extend(valid_starts)\n",
        "\n",
        "    valid_indices = np.array(valid_indices)\n",
        "    np.random.shuffle(valid_indices)\n",
        "\n",
        "    print(f\"âœ… Found {len(valid_indices)} valid sequences.\")\n",
        "\n",
        "    # 4. Train/Test Split\n",
        "    split_idx = int(len(valid_indices) * Config.TRAIN_TEST_SPLIT)\n",
        "    train_idx = valid_indices[:split_idx]\n",
        "    test_idx = valid_indices[split_idx:]\n",
        "\n",
        "    print(f\"   Training Batches: {len(train_idx) // Config.BATCH_SIZE}\")\n",
        "    print(f\"   Testing Batches:  {len(test_idx) // Config.BATCH_SIZE}\")\n",
        "\n",
        "    train_gen = TimeSeriesGenerator(df, train_idx, Config.BATCH_SIZE)\n",
        "    test_gen = TimeSeriesGenerator(df, test_idx, Config.BATCH_SIZE)\n",
        "\n",
        "    # 5. Build & Train\n",
        "    model = build_differential_model()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint(Config.get_model_path(), save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    print(\"ðŸ”¥ Starting GPU Training...\")\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=test_gen,\n",
        "        epochs=Config.EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # 6. Evaluate\n",
        "    evaluator = Evaluator(model, scaler)\n",
        "    metrics = evaluator.evaluate_and_plot(test_gen)\n",
        "\n",
        "    print(\"\\nâœ… Engine Run Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pt6FE6ag-EI",
        "outputId": "d61a4765-34fa-4893-91e5-c62116f58a4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/pipelines/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Create Root Entry Point (%%writefile main.py)\n",
        "%%writefile main.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# FORCE: Add the current directory to Python's path so it finds 'wpf_engine'\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from wpf_engine.pipelines.run import main\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"ðŸ”§ System Root: {os.getcwd()}\")\n",
        "    print(f\"ðŸ”§ Target Dataset: {Config.DATA_PATH}\")\n",
        "\n",
        "    # Run the full pipeline\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNd5uz5Bgl_5",
        "outputId": "01c474ed-9bbc-490e-fd8f-e64a9fa0e6b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Apply Float32 Fix (%%writefile wpf_engine/data/processor.py)\n",
        "%%writefile wpf_engine/data/processor.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "class PhysicsEngine:\n",
        "    @staticmethod\n",
        "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        print(\"âš¡ Igniting Physics Engine...\")\n",
        "\n",
        "        # 1. Clean Data\n",
        "        df['Patv'] = df['Patv'].clip(lower=0)\n",
        "\n",
        "        # 2. Vector Decomposition\n",
        "        wdir_rad = np.deg2rad(df['Wdir'])\n",
        "        df['U'] = df['Wspd'] * np.cos(wdir_rad)\n",
        "        df['V'] = df['Wspd'] * np.sin(wdir_rad)\n",
        "\n",
        "        # 3. Energy Flux\n",
        "        df['Energy_Flux'] = np.power(df['Wspd'], 3)\n",
        "\n",
        "        # 4. Momentum\n",
        "        df['Momentum'] = df.groupby('TurbID')['Wspd'].diff().fillna(0)\n",
        "\n",
        "        # 5. Fill NaNs\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        return df\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        print(\"âš–ï¸ Scaling 4.7M+ rows (MinMax)...\")\n",
        "        # Scale only the physics columns\n",
        "        df[Config.PHYSICS_FEATURES] = self.scaler.fit_transform(df[Config.PHYSICS_FEATURES])\n",
        "        joblib.dump(self.scaler, Config.get_scaler_path())\n",
        "        return df, self.scaler\n",
        "\n",
        "class TimeSeriesGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Fixed Generator: Explicitly extracts float32 data to prevent Object dtype errors.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, indices, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs) # Fixes the UserWarning\n",
        "        self.indices = indices\n",
        "        self.batch_size = batch_size\n",
        "        self.lookback = Config.LOOKBACK_STEPS\n",
        "        self.forecast = Config.FORECAST_STEPS\n",
        "\n",
        "        # --- CRITICAL FIX ---\n",
        "        # 1. We only extract the specific columns defined in settings\n",
        "        # 2. We explicitly cast to float32 to satisfy the GPU\n",
        "        self.data_matrix = df[Config.PHYSICS_FEATURES].values.astype(np.float32)\n",
        "\n",
        "        # Map column names to indices in this new compact matrix\n",
        "        self.feature_indices = [Config.PHYSICS_FEATURES.index(c) for c in Config.PHYSICS_FEATURES]\n",
        "        self.target_idx = Config.PHYSICS_FEATURES.index(Config.TARGET_COL)\n",
        "        # --------------------\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Batch of start indices\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "        # Pre-allocate arrays for speed (Vectorization)\n",
        "        X_hist = np.empty((self.batch_size, self.lookback, len(self.feature_indices)), dtype=np.float32)\n",
        "        X_anchor = np.empty((self.batch_size, 1), dtype=np.float32)\n",
        "        Y_future = np.empty((self.batch_size, self.forecast), dtype=np.float32)\n",
        "\n",
        "        for k, i in enumerate(batch_indices):\n",
        "            # 1. Historical Context\n",
        "            X_hist[k] = self.data_matrix[i : i + self.lookback]\n",
        "\n",
        "            # 2. Power Anchor\n",
        "            X_anchor[k] = self.data_matrix[i + self.lookback - 1, self.target_idx]\n",
        "\n",
        "            # 3. Future Target\n",
        "            Y_future[k] = self.data_matrix[i + self.lookback : i + self.lookback + self.forecast, self.target_idx]\n",
        "\n",
        "        return [X_hist, X_anchor], Y_future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip91x4Z3hcV6",
        "outputId": "8e5cb625-94be-47c3-aef9-4a4667aea8c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/data/processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Apply Dictionary Input Fix (%%writefile wpf_engine/data/processor.py)\n",
        "%%writefile wpf_engine/data/processor.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "class PhysicsEngine:\n",
        "    @staticmethod\n",
        "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        print(\"âš¡ Igniting Physics Engine...\")\n",
        "        df['Patv'] = df['Patv'].clip(lower=0)\n",
        "        wdir_rad = np.deg2rad(df['Wdir'])\n",
        "        df['U'] = df['Wspd'] * np.cos(wdir_rad)\n",
        "        df['V'] = df['Wspd'] * np.sin(wdir_rad)\n",
        "        df['Energy_Flux'] = np.power(df['Wspd'], 3)\n",
        "        df['Momentum'] = df.groupby('TurbID')['Wspd'].diff().fillna(0)\n",
        "        df = df.ffill().bfill()\n",
        "        return df\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        print(\"âš–ï¸ Scaling 4.7M+ rows (MinMax)...\")\n",
        "        df[Config.PHYSICS_FEATURES] = self.scaler.fit_transform(df[Config.PHYSICS_FEATURES])\n",
        "        joblib.dump(self.scaler, Config.get_scaler_path())\n",
        "        return df, self.scaler\n",
        "\n",
        "class TimeSeriesGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, indices, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.indices = indices\n",
        "        self.batch_size = batch_size\n",
        "        self.lookback = Config.LOOKBACK_STEPS\n",
        "        self.forecast = Config.FORECAST_STEPS\n",
        "\n",
        "        # 1. Select only physics columns\n",
        "        # 2. Force float32 to prevent Object dtype errors\n",
        "        self.data_matrix = df[Config.PHYSICS_FEATURES].values.astype(np.float32)\n",
        "\n",
        "        self.feature_indices = [Config.PHYSICS_FEATURES.index(c) for c in Config.PHYSICS_FEATURES]\n",
        "        self.target_idx = Config.PHYSICS_FEATURES.index(Config.TARGET_COL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "        X_hist = np.empty((self.batch_size, self.lookback, len(self.feature_indices)), dtype=np.float32)\n",
        "        X_anchor = np.empty((self.batch_size, 1), dtype=np.float32)\n",
        "        Y_future = np.empty((self.batch_size, self.forecast), dtype=np.float32)\n",
        "\n",
        "        for k, i in enumerate(batch_indices):\n",
        "            X_hist[k] = self.data_matrix[i : i + self.lookback]\n",
        "            X_anchor[k] = self.data_matrix[i + self.lookback - 1, self.target_idx]\n",
        "            Y_future[k] = self.data_matrix[i + self.lookback : i + self.lookback + self.forecast, self.target_idx]\n",
        "\n",
        "        # FIX: Return Dictionary mapping input layer names -> Data\n",
        "        # This matches names defined in wpf_engine/models/architecture.py\n",
        "        inputs = {\n",
        "            \"history_in\": X_hist,\n",
        "            \"anchor_in\": X_anchor\n",
        "        }\n",
        "\n",
        "        return inputs, Y_future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFugYrUifcL",
        "outputId": "20e9dca1-df85-48f1-ef09-a0a4f7e70fe4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/data/processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Optimize Settings (%%writefile wpf_engine/config/settings.py)\n",
        "%%writefile wpf_engine/config/settings.py\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    # -- Data Paths --\n",
        "    DATA_PATH = \"wtbdata_245days.csv\"\n",
        "    ARTIFACTS_DIR = \"saved_models\"\n",
        "    PLOT_DIR = os.path.join(ARTIFACTS_DIR, \"plots\")\n",
        "\n",
        "    # -- Physics & Time --\n",
        "    LOOKBACK_STEPS = 144\n",
        "    FORECAST_STEPS = 288\n",
        "\n",
        "    # -- OPTIMIZATION SETTINGS --\n",
        "    BATCH_SIZE = 2048      # INCREASED 8x (From 256)\n",
        "    EPOCHS = 30\n",
        "    LEARNING_RATE = 1e-3\n",
        "    TRAIN_TEST_SPLIT = 0.9\n",
        "\n",
        "    # -- Feature Definitions --\n",
        "    TARGET_COL = \"Patv\"\n",
        "    PHYSICS_FEATURES = [\n",
        "        \"Wspd\", \"Wdir\", \"Etmp\", \"Itmp\", \"Ndir\", \"Pab1\", \"Pab2\", \"Pab3\", \"Prtv\",\n",
        "        \"Patv\", \"U\", \"V\", \"Energy_Flux\", \"Momentum\"\n",
        "    ]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"deep_physics_transformer.h5\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_scaler_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"global_scaler.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwjN_gJljYXx",
        "outputId": "42ca7e8c-0cf8-42ee-a7f0-0f4f82a86681"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/config/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Apply Vectorized Generator (%%writefile wpf_engine/data/processor.py)\n",
        "%%writefile wpf_engine/data/processor.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from wpf_engine.config.settings import Config\n",
        "\n",
        "class PhysicsEngine:\n",
        "    @staticmethod\n",
        "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        print(\"âš¡ Igniting Physics Engine...\")\n",
        "        df['Patv'] = df['Patv'].clip(lower=0)\n",
        "        wdir_rad = np.deg2rad(df['Wdir'])\n",
        "        df['U'] = df['Wspd'] * np.cos(wdir_rad)\n",
        "        df['V'] = df['Wspd'] * np.sin(wdir_rad)\n",
        "        df['Energy_Flux'] = np.power(df['Wspd'], 3)\n",
        "        df['Momentum'] = df.groupby('TurbID')['Wspd'].diff().fillna(0)\n",
        "        df = df.ffill().bfill()\n",
        "        return df\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        print(\"âš–ï¸ Scaling 4.7M+ rows (MinMax)...\")\n",
        "        df[Config.PHYSICS_FEATURES] = self.scaler.fit_transform(df[Config.PHYSICS_FEATURES])\n",
        "        joblib.dump(self.scaler, Config.get_scaler_path())\n",
        "        return df, self.scaler\n",
        "\n",
        "class TimeSeriesGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Highly Optimized Vectorized Generator.\n",
        "    Removes Python loops to maximize GPU throughput.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, indices, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.indices = indices\n",
        "        self.batch_size = batch_size\n",
        "        self.lookback = Config.LOOKBACK_STEPS\n",
        "        self.forecast = Config.FORECAST_STEPS\n",
        "\n",
        "        # Matrix is loaded into RAM once\n",
        "        self.data_matrix = df[Config.PHYSICS_FEATURES].values.astype(np.float32)\n",
        "        self.target_idx = Config.PHYSICS_FEATURES.index(Config.TARGET_COL)\n",
        "\n",
        "        # Pre-calculate offset arrays for broadcasting\n",
        "        # This creates the \"sliding window\" indices in C-speed\n",
        "        self.lookback_offsets = np.arange(self.lookback)\n",
        "        self.forecast_offsets = np.arange(self.forecast) + self.lookback\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 1. Get the start indices for this batch\n",
        "        starts = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "        # 2. Vectorized Slicing (Magic happens here)\n",
        "        # Create a grid of indices: (Batch_Size, Lookback_Steps)\n",
        "        # This replaces the 'for' loop entirely\n",
        "        history_idx = starts[:, None] + self.lookback_offsets\n",
        "        forecast_idx = starts[:, None] + self.forecast_offsets\n",
        "        anchor_idx = starts + self.lookback - 1\n",
        "\n",
        "        # 3. Bulk Fetch from RAM\n",
        "        X_hist = self.data_matrix[history_idx] # Shape: (Batch, 144, Features)\n",
        "        X_anchor = self.data_matrix[anchor_idx, self.target_idx] # Shape: (Batch,)\n",
        "        Y_future = self.data_matrix[forecast_idx, self.target_idx] # Shape: (Batch, 288)\n",
        "\n",
        "        # 4. Reshape Anchor to (Batch, 1) to match Model Input\n",
        "        X_anchor = X_anchor[:, None]\n",
        "\n",
        "        inputs = {\n",
        "            \"history_in\": X_hist,\n",
        "            \"anchor_in\": X_anchor\n",
        "        }\n",
        "\n",
        "        return inputs, Y_future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v85SrTFjcBR",
        "outputId": "fd6a0c15-3a51-47c9-9705-3d24dd672221"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/data/processor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Update Pipeline (%%writefile wpf_engine/pipelines/run.py)\n",
        "%%writefile wpf_engine/pipelines/run.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# --- PATH PATCH ---\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "root_dir = os.path.abspath(os.path.join(current_dir, '../..'))\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "# ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.data.processor import PhysicsEngine, DataManager, TimeSeriesGenerator\n",
        "from wpf_engine.models.architecture import build_differential_model\n",
        "from wpf_engine.core.evaluator import Evaluator\n",
        "\n",
        "def main():\n",
        "    print(f\"ðŸš€ Initializing HIGH-PERFORMANCE Engine (Batch Size: {Config.BATCH_SIZE})...\")\n",
        "\n",
        "    # Load & Process\n",
        "    try:\n",
        "        df = pd.read_csv(Config.DATA_PATH)\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ Dataset not found.\")\n",
        "        return\n",
        "\n",
        "    df = PhysicsEngine.engineer_features(df)\n",
        "    manager = DataManager()\n",
        "    df, scaler = manager.prepare_data(df)\n",
        "\n",
        "    # Indices Logic\n",
        "    print(\"âš™ï¸ Computing valid sequence indices...\")\n",
        "    groups = df.groupby('TurbID')\n",
        "    valid_indices = []\n",
        "    total_len = Config.LOOKBACK_STEPS + Config.FORECAST_STEPS\n",
        "\n",
        "    for turb_id, group in groups:\n",
        "        g_indices = group.index.values\n",
        "        n_samples = len(g_indices) - total_len\n",
        "        if n_samples > 0:\n",
        "            valid_indices.extend(g_indices[:n_samples])\n",
        "\n",
        "    valid_indices = np.array(valid_indices)\n",
        "    np.random.shuffle(valid_indices)\n",
        "    print(f\"âœ… Found {len(valid_indices)} valid sequences.\")\n",
        "\n",
        "    # Split\n",
        "    split_idx = int(len(valid_indices) * Config.TRAIN_TEST_SPLIT)\n",
        "    train_idx = valid_indices[:split_idx]\n",
        "    test_idx = valid_indices[split_idx:]\n",
        "\n",
        "    # Generators\n",
        "    train_gen = TimeSeriesGenerator(df, train_idx, Config.BATCH_SIZE)\n",
        "    test_gen = TimeSeriesGenerator(df, test_idx, Config.BATCH_SIZE)\n",
        "\n",
        "    # Model\n",
        "    model = build_differential_model()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "        ModelCheckpoint(Config.get_model_path(), save_best_only=True, monitor='val_loss'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n",
        "    ]\n",
        "\n",
        "    print(f\"ðŸ”¥ Starting GPU Training (Batch Size {Config.BATCH_SIZE})...\")\n",
        "\n",
        "    # --- PERFORMANCE FIX: Multiprocessing ---\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=test_gen,\n",
        "        epochs=Config.EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        workers=4,                # Use 4 CPU cores to fetch data\n",
        "        use_multiprocessing=True, # Parallelize the Generator\n",
        "        max_queue_size=20         # Buffer more batches in RAM\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = Evaluator(model, scaler)\n",
        "    metrics = evaluator.evaluate_and_plot(test_gen)\n",
        "    print(\"\\nâœ… Engine Run Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o28y7VcQjewq",
        "outputId": "72411678-2daf-40e6-9f79-f1b309e48323"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/pipelines/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Fix Keras 3 Compatibility (%%writefile wpf_engine/pipelines/run.py)\n",
        "%%writefile wpf_engine/pipelines/run.py\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# --- PATH PATCH ---\n",
        "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "root_dir = os.path.abspath(os.path.join(current_dir, '../..'))\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "# ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from wpf_engine.config.settings import Config\n",
        "from wpf_engine.data.processor import PhysicsEngine, DataManager, TimeSeriesGenerator\n",
        "from wpf_engine.models.architecture import build_differential_model\n",
        "from wpf_engine.core.evaluator import Evaluator\n",
        "\n",
        "def main():\n",
        "    print(f\"ðŸš€ Initializing HIGH-PERFORMANCE Engine (Batch Size: {Config.BATCH_SIZE})...\")\n",
        "\n",
        "    # Load & Process\n",
        "    try:\n",
        "        df = pd.read_csv(Config.DATA_PATH)\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ Dataset not found.\")\n",
        "        return\n",
        "\n",
        "    df = PhysicsEngine.engineer_features(df)\n",
        "    manager = DataManager()\n",
        "    df, scaler = manager.prepare_data(df)\n",
        "\n",
        "    # Indices Logic\n",
        "    print(\"âš™ï¸ Computing valid sequence indices...\")\n",
        "    groups = df.groupby('TurbID')\n",
        "    valid_indices = []\n",
        "    total_len = Config.LOOKBACK_STEPS + Config.FORECAST_STEPS\n",
        "\n",
        "    for turb_id, group in groups:\n",
        "        g_indices = group.index.values\n",
        "        n_samples = len(g_indices) - total_len\n",
        "        if n_samples > 0:\n",
        "            valid_indices.extend(g_indices[:n_samples])\n",
        "\n",
        "    valid_indices = np.array(valid_indices)\n",
        "    np.random.shuffle(valid_indices)\n",
        "    print(f\"âœ… Found {len(valid_indices)} valid sequences.\")\n",
        "\n",
        "    # Split\n",
        "    split_idx = int(len(valid_indices) * Config.TRAIN_TEST_SPLIT)\n",
        "    train_idx = valid_indices[:split_idx]\n",
        "    test_idx = valid_indices[split_idx:]\n",
        "\n",
        "    # Generators (Vectorized = Fast)\n",
        "    train_gen = TimeSeriesGenerator(df, train_idx, Config.BATCH_SIZE)\n",
        "    test_gen = TimeSeriesGenerator(df, test_idx, Config.BATCH_SIZE)\n",
        "\n",
        "    # Model\n",
        "    model = build_differential_model()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "        ModelCheckpoint(Config.get_model_path(), save_best_only=True, monitor='val_loss'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n",
        "    ]\n",
        "\n",
        "    print(f\"ðŸ”¥ Starting GPU Training (Batch Size {Config.BATCH_SIZE})...\")\n",
        "\n",
        "    # --- KERAS 3 FIX: Removed 'workers' and 'use_multiprocessing' ---\n",
        "    # The Vectorized Generator is fast enough to run on the main thread\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=test_gen,\n",
        "        epochs=Config.EPOCHS,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = Evaluator(model, scaler)\n",
        "    metrics = evaluator.evaluate_and_plot(test_gen)\n",
        "    print(\"\\nâœ… Engine Run Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsLS_5v8kCoc",
        "outputId": "3a166366-6230-4e9a-e21f-53b8d67cf2cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/pipelines/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Maximize Settings (%%writefile wpf_engine/config/settings.py)\n",
        "%%writefile wpf_engine/config/settings.py\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    # -- Data Paths --\n",
        "    DATA_PATH = \"wtbdata_245days.csv\"\n",
        "    ARTIFACTS_DIR = \"saved_models\"\n",
        "    PLOT_DIR = os.path.join(ARTIFACTS_DIR, \"plots\")\n",
        "\n",
        "    # -- Physics & Time --\n",
        "    LOOKBACK_STEPS = 144\n",
        "    FORECAST_STEPS = 288\n",
        "\n",
        "    # -- ULTRA SETTINGS --\n",
        "    BATCH_SIZE = 4096      # Pushing to 4096 to fill VRAM\n",
        "    EPOCHS = 30\n",
        "    LEARNING_RATE = 5e-4   # Slightly lower LR for larger batches\n",
        "    TRAIN_TEST_SPLIT = 0.9\n",
        "\n",
        "    # -- Feature Definitions --\n",
        "    TARGET_COL = \"Patv\"\n",
        "    PHYSICS_FEATURES = [\n",
        "        \"Wspd\", \"Wdir\", \"Etmp\", \"Itmp\", \"Ndir\", \"Pab1\", \"Pab2\", \"Pab3\", \"Prtv\",\n",
        "        \"Patv\", \"U\", \"V\", \"Energy_Flux\", \"Momentum\"\n",
        "    ]\n",
        "\n",
        "    @classmethod\n",
        "    def get_model_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"deep_physics_transformer.h5\")\n",
        "\n",
        "    @classmethod\n",
        "    def get_scaler_path(cls):\n",
        "        return os.path.join(cls.ARTIFACTS_DIR, \"global_scaler.pkl\")"
      ],
      "metadata": {
        "id": "rtj1cw1LoOaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abb0667-4e04-4da5-97e9-5f7101856d1d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wpf_engine/config/settings.py\n"
          ]
        }
      ]
    }
  ]
}